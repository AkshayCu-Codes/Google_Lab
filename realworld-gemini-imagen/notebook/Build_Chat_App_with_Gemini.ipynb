{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b27dad",
   "metadata": {},
   "source": [
    "\n",
    "# Build an Application to Send Chat Prompts using the Gemini Model\n",
    "\n",
    "This notebook walks through how to interact with the Gemini generative AI model on Vertex AI, using both standard and streaming chat prompts.\n",
    "\n",
    "## Objectives\n",
    "- Connect to Vertex AI\n",
    "- Load the Gemini model\n",
    "- Send prompts and receive chat responses (with and without streaming)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc45b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install necessary packages (uncomment if running locally)\n",
    "# !pip install google-cloud-aiplatform google-generativeai\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37024ef",
   "metadata": {},
   "source": [
    "## Chat Without Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d3713",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google import genai\n",
    "from google.genai.types import HttpOptions, ModelContent, Part, UserContent\n",
    "\n",
    "# Initialize client for Vertex AI Gemini model\n",
    "client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project=\"your-project-id\",\n",
    "    location=\"your-region\",\n",
    "    http_options=HttpOptions(api_version=\"v1\")\n",
    ")\n",
    "\n",
    "# Create chat session with history\n",
    "chat = client.chats.create(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    history=[\n",
    "        UserContent(parts=[Part(text=\"Hello\")]),\n",
    "        ModelContent(parts=[Part(text=\"Great to meet you. What would you like to know?\")]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Send a message and print the response\n",
    "response = chat.send_message(\"What are all the colors in a rainbow?\")\n",
    "print(\"Response 1:\", response.text)\n",
    "\n",
    "response = chat.send_message(\"Why does it appear when it rains?\")\n",
    "print(\"Response 2:\", response.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc6c0ec",
   "metadata": {},
   "source": [
    "## Chat With Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f33568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google import genai\n",
    "from google.genai.types import HttpOptions\n",
    "\n",
    "# Initialize client for Vertex AI Gemini model (again for demonstration)\n",
    "client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project=\"your-project-id\",\n",
    "    location=\"your-region\",\n",
    "    http_options=HttpOptions(api_version=\"v1\")\n",
    ")\n",
    "\n",
    "# Create chat session\n",
    "chat = client.chats.create(model=\"gemini-2.0-flash-001\")\n",
    "\n",
    "# Send streaming message\n",
    "response_text = \"\"\n",
    "for chunk in chat.send_message_stream(\"What are all the colors in a rainbow?\"):\n",
    "    print(chunk.text, end=\"\")\n",
    "    response_text += chunk.text\n",
    "\n",
    "# Optionally use response_text further\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c343199",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Note**: Make sure to replace `\"your-project-id\"` and `\"your-region\"` with your actual GCP project ID and region.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "- Used Gemini model to send chat prompts.\n",
    "- Compared streaming and non-streaming response mechanisms.\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
